\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{minibox}
\usepackage{listings}
\usepackage{verbatim}
\lstset{
  language=bash,
  basicstyle=\ttfamily
}
% \usepackage{pdf14} % Enable for Manuscriptcentral -- can't handle pdf 1.5
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some submissions.



\usepackage{natbib}
\bibliographystyle{rusnat}




\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=black,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=black
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}


\begin{document}
\title{%
  EPP Final Project \\
  \large Panel Study Labour Market and Social Security Campus File (PASS-CF) Data Preparation\thanks{Aysu Avci, Melih Damar, University of Bonn. Email: \href{mailto:aysuavci.s@gmail.com}{\nolinkurl{aysuavci [dot] s [at] gmail [dot] com}}.}}

\author{Aysu Avci, Melih Damar}

\date{
    {\bf Preliminary -- please do not quote}
    \\[1ex]
    \today
}

\maketitle

\begin{abstract}
    The purpose of this project is to create a PASS-CF data preparation repository that can be a template and a starting point for a similar repository for the main PASS data set. By means of this project, we aim to familiarize ourselves with the effective use of programming in cleaning panel data sets and performing initial analysis. The following is the structure of the project paper: First, we will provide a brief overview of the PASS-CF dataset and repository structure, followed by more detailed information about the data preparation steps, and finally, we will present some example summary statistics that we have generated.
\end{abstract}
\clearpage


\section{Introduction}\label{section:intro}


The Panel Study Labour Market and Social Security (PASS) is a dataset established in 2007 by the Institute for Employment Research (IAB) among German households. The dataset contains information at the household and individual levels. Households are identified via  hnr  and  wave . Individuals are identified via  pnr  and  wave . Access to the PASS main dataset is only possible via an application to the Research Data Center (FDZ).\\[12pt]
The PASS Campus file (PASS-CF) dataset is a simplified version of the main dataset that is suitable for academic teaching and obtaining various insights into the handling of PASS data. Compared to the main dataset, PASS-CF contains a reduced number of observations, range of variables and modified identification numbers as well as information; therefore it is not suitable for substantial scientific analysis. The dataset PASS-CF is accessable after filling the form in the following link: \url{https://fdz.iab.de/en/campus-files/pass_cf/registrierungsformular-zum-download-des-campus-files-pass-0617-v1.aspx}. \\[12pt]
The following longitudinal PASS-CF datasets are being used in this project:
\begin{itemize}
  \item HHENDDAT\_cf\_W11.dta
  \item PENDDAT\_cf\_W11.dta
  \item hweights\_cf\_W11.dta
  \item pweights\_cf\_W11.dta
\end{itemize}
Therefore, please add these data files into the folder \emph{'src$/$original-data$/$'} in your local repository on your computer. Please make sure you have your conda environment up to date. The basic requirements can be found in the \emph{'environment.yml'} file. The output files are created by running the following:

\begin{lstlisting}
conda develop .
pytask
\end{lstlisting}

This resource can be helpful to get an understanding of pytask: \url{https://pytask-dev.readthedocs.io/en/latest/index.html}

\subsection{Repository Structure}
This repository follows the project template from \citet{GaudeckerEconProjectTemplates}.
-  \emph {src$/$original\_data$/$} should contain the four dataset that are added to the folder by the user. For each \emph {data\_set} there should be a  \emph {\{data\_set\}\_renaming.csv} in the \emph {src$/$data\_management$/$}.\\[12pt]
- \emph {src$/$data\_management$/$} contains all the files related to cleaning process. The functions used for cleaning steps can be found in the file \emph {cleaning\_functions.py}. As mentioned above this file also contains the renaming documents under each \emph {data\_set$/$} folder. The creation of dummy variables requires a list of variables that will be used in the \emph {create\_dummies()} function, therefore in \emph {dummies$/$} each \emph {data\_set} that requires such operation should have a \emph {\{data\_set\}\_dummies.yaml}. The tests written for cleaning functions are in the \emph {test\_cleaning.py} file. And finally, the cleaning task itself can be found in \emph {task\_cleaning.py} which creates the new datasets in three steps.\\[12pt]
- After running the pytask the final data sets \emph {PENDDAT\_aggregated.pickle} and \emph {HHENDDAT\_aggregated.pickle} are created under \emph {bld$/$}, as well as a merged alternative of the datasets \emph {merge\_clean.pickle}.\\[12pt]
- \emph {src$/$final$/$} contains \emph {task\_stat.py}, the task needed to form summary statistics.\\[12pt]
- Other tasks include \emph {task\_documentation.py} and \emph {task\_paper.py} which forms the \\ \emph {research\_project.pdf} based on \emph {research\_paper.tex} and \emph {\{data\_set\}\_sum\_stat.tex}.

\section{Implementation Details}\label{section:imp}
The script performs the following steps for both household and individual level datasets.
\begin{enumerate}
\item Collect the respective .dta file.
\item Rename all variables according to the respective renaming .csv file.
\item Perform basic data cleaning.
\item Reverse coding variables and aggregation.
\item Create dummies that might come in useful.
\item Merge the datasets.
\item Save the final data sets as .pickle.
\item Report some summary statistics and create research paper in pdf format.
\end{enumerate}
All the data cleaning steps,from step 1 to 7, are specified in  \emph {src$/$data\_management$/$task\_cleaning.py}.
The detailed information about all of the steps can be found below.

\subsection{Renaming Files}
For each \emph {data\_set} there should be a \emph {\{data\_set\}\_renaming.csv} in the  \emph {src$/$data\_management$/$}.
The \emph {\{data\_set\}\_renaming.csv} files with an empty new variable name column are formed using \emph {create\_renaming\_file()} function which can be found in  \emph {src$/$sandbox$/$create\_renaming\_file.ipynb}. \\[12pt]
The renaming files are ";"-separated .csv files and specify the new name for each variable.\\[12pt]
Since the respective .csv files contains all the variables in that dataset with the new variable names, it might be an useful documentation to view all the variables.\\[12pt]
The general information about the original naming of the datasets can be found in Table 21 of the PASS User Guide which can be dowloaded via the following link: \url{https://doku.iab.de/fdz/pass/FDZ-Datenreporte_PASS_EN.zip}.\\[12pt]
Some of the standardizations we use in renaming:

\subsection{Basic Data Cleaning}
\begin{enumerate}
\item Use of English
\item A common naming for the variables in the same module (e.g.  big\_5 ).
\item All the negatively phrased variables \footnote{Referring to the items in a scale that differ in direction from most other items in that scale.} ends with  \_n .
\end{enumerate}
\subsection{Reverse Coding and Aggregation}
New variables are created according to the \href{https://doku.iab.de/fdz/reporte/2020/MR_07-20_EN.pdf}{PASS Scale and Instrument Manual}.\\[12pt]
Like deprivation module in the household level data, some variables are already aggregated and can be found in the data. We extent this practice to the following modules in the individual level data:
\begin{enumerate}
\item Big Five
\item Effort-Reward Imbalance Scale (ERI Scale)
\item Gender Role Attitudes
\end{enumerate}
For all the modules, the negatively phrased variables are inverted before the aggregation and newly created variables are named according to module name.

\subsection{Creating Dummy Variables}
All the variables we use to create dummies are specified in:
\\ \emph {src$/$data\_management$/$dummies$/$\{data\_name\}\_dummies.yaml}\\
Dummy variables are created without changing the original variables or values. For convenience, we create dummy variable in the following structure \emph {\{original\_variable\_name\}\_dummy}.\\[12pt]
In PASS\-CF dataset, the questions with two possible answers were not coded as dummy variables but variables consist of values 1 and 2 (e.g. Yes=1, No=2). Therefore, we create dummy variables for the following type of items:
\begin{enumerate}
\item  Yes/No questions (e.g. social media usage in the last 4 weeks)
\item Categorical questions with two possible answers (e.g. gender).
\end{enumerate}
On top of these variables we also created dummies for:
\begin{enumerate}
\setcounter{enumi}{3}
\item \emph {PG0100}, a numeric variable that ranges between 0\-99 and indicates the number of doctor visits in the last 3 months.
\item Financial reason dummies for the Deprivation Module. In this module, individuals were asked about owning certain goods or engaging in certain activities. In case the household answers no to an item, the household is asked if it is due to financial or other reasons. Therefore, we create dummies where the value 1 corresponds to not owning goods or engaging in activities for financial reasons (e.g., no car for financial reasons).
\end{enumerate}
\subsection{Task Cleaning and Merging Datasets}
The \emph {task\_cleaning.py} is divided into three steps and at the end of each step a file with processed datasets are formed:
\begin{enumerate}
\item  \emph {task\_basic\_cleaning} performs renaming, basic cleaning and indexing for each \emph {data\_set}; and returns \emph {\{data\_set\}\_clean.pickle} to \emph {bld$/$cleaned\_data$/$}.
\item \emph {task\_aggregation\_and\_dummy} performs reverse coding, creating aggregated variables and dummy variables for \emph {PENDDAT} and \emph {HHENDDAT} and returns\\
\emph {\{data\_set\}\_aggregated.pickle} to \emph {bld$/$aggregated\_data$/$}.
\item \emph {task\_merging} first merges the aggregated \emph {PENDDAT} and \emph {HHENDDAT} datasets with their cleaned weight datasets \emph {hweights} and \emph {pweights} and produces the two\\ \emph {\{data\_set\}\_weighted.pickle} to \emph {bld$/$weighted\_data$/$}. Secondly, it merges this two weighted datasets and created \emph {merged\_clean.pickle} under \emph {bld$/$final\_data}.
\end{enumerate}
We did not delete any of the newly formed dataset files during the intermediate steps to allow researchers to use their preferred dataset. However, we added lines of code at the end of the \emph {task\_merging} that would enable researchers to delete the datasets formed in the intermediate steps before they are created in the \emph {bld$/$}.
\clearpage
\section{Summary Statistics}\label{section:stats}
We decide to include some summary statistics to provide an understanding of how the newly generated data can be used and integrated into our template. All the tasks used for creating plots and tables are in the \emph{src$/$final$/$} folder.\\[12pt]
\begin{figure}[htbp]
    \centerline{\includegraphics[width=20cm,scale=0.1]{../../bld/figures/number_obs.png}}
    \end{figure}
\clearpage
\subsection{Household Level Data Specific}
The tables are created by \emph{src$/$final$/$task\_stat.py} using \emph {\{data\_set\}\_stat.yaml}. If another variable is to be added to the tables, this can be done by adding the original variable name and the desired variable name.
\begin{footnotesize}
\begin{center}
\input{../../bld/paper/HHENDDAT_stat.tex}
\end{center}
\end{footnotesize}

\subsection{Individual Level Data Specific}
\begin{footnotesize}
\begin{center}
\input{../../bld/paper/PENDDAT_stat.tex}
\end{center}
\end{footnotesize}

\begin{figure}[!htb]
    \centerline{\includegraphics[width=20cm,scale=0.1]{../../bld/figures/age_dist.png}}
    \caption{\label{fig:1}}
    \end{figure}


\begin{figure}[h!]
    \centerline{\includegraphics[width=11cm,scale=0.1]{../../bld/figures/gender_dist.png}}
    \caption{\label{fig:2}}
    \end{figure}

\begin{figure}[h!]
    \centerline{\includegraphics[width=20cm,scale=0.1]{../../bld/figures/gender_role.png}}
    \caption{\label{fig:3}}
    \end{figure}

% section introduction (end)





\bibliography{refs}



% \appendix

% The chngctr package is needed for the following lines.
% \counterwithin{table}{section}
% \counterwithin{figure}{section}

\end{document}